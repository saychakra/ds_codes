# Linear Regression

## What does this code do?
Implements linear regression from scratch using gradient descent. This is a foundational machine learning algorithm that finds the best-fit line through data points by minimizing the sum of squared errors.

## Key Components
- **Data Generation**: Creates synthetic linear data with noise
- **Hypothesis Function**: y = mx + b (linear equation)
- **Cost Function**: Mean Squared Error (MSE) - measures prediction error
- **Gradient Descent**: Iteratively updates weights to minimize cost

## Learning Concepts
- Linear regression fundamentals
- Gradient descent optimization
- Cost functions and loss
- Model fitting and convergence

## Visual Overview
```
Data points → Initialize weights → Calculate predictions 
→ Compute error → Update weights → Repeat until convergence
```

## Related Files
- `logistic_regression.py` - Classification variant
- `regularizations.py` - Adding regularization terms
- `gradient_descent.ipynb` - More detailed gradient descent exploration

## Further Reading
- Understand how gradient descent works
- Cost function derivatives and backpropagation
- Convergence criteria and learning rates
